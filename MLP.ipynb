{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80d4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from d2l import torch as d2l\n",
    "#23/05/2023 - done with the most basic form of MLP implementation\n",
    "#24/05/2023 - fixed wrong dimensions of inputs in Evaluation & Loss Functions and other bugs, added Dropout & Unflatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2343a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config Parameters\n",
    "quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.60, 0.70, 0.80, 0.90, 0.95]\n",
    "qlen = len(quantiles)\n",
    "i_train = 2400\n",
    "i_val = i_train + 800\n",
    "i_test = i_val + 1600 \n",
    "#2400 for training, 800 for validation, 1600 for testing\n",
    "batch_size, lr, n_epochs, num_iter = 256, 0.001, 100, 30\n",
    "lag_period, num_features, forecast_horizon = 20, 14, 2\n",
    "num_inputs, num_outputs, num_hidden = lag_period*num_features, (1+qlen)*num_features, 100\n",
    "dropout= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f340a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load & Split Data\n",
    "data = pd.read_csv(\"/Users/lixiang/Desktop/DeepJMQR Project/data.csv\")\n",
    "alldata = np.array(data)[:,1:].astype(\"float32\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "X = []\n",
    "for i in range(lag_period, len(data)):\n",
    "    X.append(alldata[i-lag_period:i])\n",
    "X = torch.tensor(np.array(X), requires_grad = True)\n",
    "Y = alldata[lag_period+forecast_horizon:]\n",
    "\n",
    "X_train, Y_train = X[:i_train], torch.tensor(Y[:i_train])\n",
    "X_val, Y_val = X[i_train:i_val], Y[i_train:i_val]\n",
    "X_test, Y_test = X[i_val:i_test], Y[i_val:i_test]\n",
    "train_iter = torch.utils.data.DataLoader(list(zip(X_train,Y_train)), batch_size=batch_size, shuffle = True)\n",
    "val_iter = torch.utils.data.DataLoader(list(zip(X_val,Y_val)), batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b17ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation & Loss Functions\n",
    "def lossfn(τ, y, ŷ):\n",
    "    #for ConvLSTM & MLP\n",
    "    #τ: quantile vector of length J\n",
    "    #ŷ: prediction, 3D tensor of dim B x M x (1+J)\n",
    "    #y: observation, 2D tensor of dim B x M\n",
    "    #B = batch size or test/val data size\n",
    "    loss = torch.sum(torch.square(y-ŷ[:,:,0]))\n",
    "    for i in range(len(τ)):\n",
    "        q = τ[i]\n",
    "        r = y - ŷ[:,:,i+1]\n",
    "        loss += torch.sum(q*r - r*(r<0))\n",
    "    loss /= y.shape[0]\n",
    "    return loss\n",
    "\n",
    "#for evaluation: remember to turn tensors into np.array()\n",
    "def tilted_loss(τ, y, ŷ):\n",
    "    #y & ŷ: np.array() of same shape as defined in lossfn\n",
    "    loss = 0.0\n",
    "    for i in range(len(τ)):\n",
    "        q = τ[i]\n",
    "        r = y - ŷ[:,:,i+1]\n",
    "        loss += np.sum(q*r - r*(r<0))\n",
    "    loss /= y.shape[0]\n",
    "    return loss\n",
    "def crossing_loss(ŷ):\n",
    "    #ŷ: np.array() of same shape as defined in lossfn\n",
    "    loss = 0.0 #crossing loss as defined in the paper\n",
    "    num_cross = 0.0\n",
    "    for i in range(len(ŷ[0,0,:])-2):\n",
    "        q = ŷ[:,:,i+1] - ŷ[:,:,i+2]\n",
    "        loss += np.sum(np.maximum(q,0))\n",
    "        num_cross += np.sum(q>0)\n",
    "    loss /= ŷ.shape[0]\n",
    "#     num_cross /= ŷ.shape[0]*ŷ.shape[1]*ŷ.shape[2]\n",
    "    return loss, num_cross\n",
    "def eval_quantiles(lower, upper, trues):\n",
    "    #all inputs are np.array of dim B x M\n",
    "    icp = np.mean((trues > lower) & (trues < upper))\n",
    "    mil = np.mean(np.maximum(0,upper-lower))\n",
    "    return icp,mil\n",
    "def eval_error(y, ŷ):\n",
    "    #y, ŷ: np.array() of same shape as defined in lossfn\n",
    "    r = np.abs(y-ŷ[:,:,0])\n",
    "    mse = np.mean(r*r)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(r)\n",
    "    return mse, rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd67fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Model\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std = 0.01)\n",
    "def init():\n",
    "    net = nn.Sequential(nn.Flatten(), \n",
    "                        nn.Linear(num_inputs,num_hidden), \n",
    "                        nn.ReLU(), \n",
    "                        nn.Dropout(dropout),\n",
    "                        nn.Linear(num_hidden,num_outputs),\n",
    "                        nn.Unflatten(1,(num_features, (1+qlen))))\n",
    "    net.apply(init_weights)\n",
    "    return net\n",
    "net = init()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = lr)\n",
    "def train(model, train_iter, quantiles, loss_fn, optimizer, num_epochs = 100):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    for i, data in enumerate(train_iter):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(quantiles, labels, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        last_loss = loss.item()\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f255e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "def iter():\n",
    "    best_vloss = 1e9\n",
    "#     animator = d2l.Animator(xlabel = \"epoch\", ylabel = \"loss\", xlim = [1,n_epochs], legend = [\"train\", \"val\"])\n",
    "    for epoch in range(n_epochs):\n",
    "        net.train(True)\n",
    "        avg_loss = train(net, train_iter, quantiles, lossfn, optimizer, num_epochs = n_epochs)\n",
    "        net.train(False)\n",
    "        running_vloss = 0.0\n",
    "        for i, vdata in enumerate(val_iter):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs =  net(vinputs)\n",
    "            vloss = lossfn(quantiles, vlabels, voutputs)\n",
    "            running_vloss += vloss\n",
    "        avg_vloss = float(running_vloss / (i+1))\n",
    "#         if epoch % (n_epochs/20) == 0:\n",
    "#             animator.add(epoch +1, (avg_loss, avg_vloss))\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            torch.save(net, \"model_best_state\")\n",
    "    model = torch.load(\"model_best_state\")\n",
    "    pred = model(X_val).detach().numpy()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9156ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate \n",
    "cl,tl,x,icp,mil = [],[],[],[[] for _ in range(qlen//2)],[[] for _ in range(qlen//2)]\n",
    "for _ in range(num_iter):\n",
    "    net = init()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)\n",
    "    pred = iter()\n",
    "    cl.append(crossing_loss(pred))\n",
    "    tl.append(tilted_loss(quantiles,Y_val,pred))\n",
    "    x.append(eval_error(Y_val,pred))\n",
    "    for i in range(qlen//2):\n",
    "        t1,t2 = eval_quantiles(pred[:,:,i+1],pred[:,:,qlen-i],Y_val)\n",
    "        icp[i].append(t1)\n",
    "        mil[i].append(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb3728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossing Loss: 3.4018411928021436e-05 , Number of Cross: 177.83333333333334\n",
      "MSE: 0.000108547065 RMSE: 0.010388785 MAE: 0.008260885\n",
      "Tilted loss: 0.18948630287249882\n",
      "Prediction Intervals:\n",
      "90.0 % ICP & MIL: 0.96278 0.030739\n",
      "80.0 % ICP & MIL: 0.882938 0.016032\n",
      "60.0 % ICP & MIL: 0.70453 0.009112\n",
      "40.0 % ICP & MIL: 0.499426 0.005204\n",
      "20.0 % ICP & MIL: 0.28058 0.002318\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Errors\n",
    "print(\"Crossing Loss:\", np.mean([y[0] for y in cl]), \", Number of Cross:\", np.mean([y[1] for y in cl]))\n",
    "print(\"MSE:\",np.mean([y[0] for y in x]), \"RMSE:\", np.mean([y[1] for y in x]),\"MAE:\", np.mean([y[2] for y in x]))\n",
    "print(\"Tilted loss:\", np.mean(tl))\n",
    "print(\"Prediction Intervals:\")\n",
    "for i in range(qlen//2):\n",
    "    print(round(quantiles[qlen-i-1]-quantiles[i],1)*100,\"% ICP & MIL:\",round(np.mean(icp[i]),6),round(np.mean(mil[i]),6))\n",
    "#Test data will be touched after everything is done i.e. tuning hyperparameters & adjusting model architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
